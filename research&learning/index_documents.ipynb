{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: e:\\Study Space\\Python Workspace\\ELastic Search\n",
      "Environment file path: e:\\Study Space\\Python Workspace\\ELastic Search\\.env.local\n",
      "Raw LOCALHOST value: 'http://localhost:9200/'\n",
      "LOCALHOST found in environment file.\n",
      "Processed LOCALHOST value: 'http://localhost:9200/'\n",
      "Connected to Elasticsearch!\n",
      "{'cluster_name': 'docker-cluster',\n",
      " 'cluster_uuid': 'AKPh90H1StWquQfBPE4Chw',\n",
      " 'name': 'b66a5ae1a4a1',\n",
      " 'tagline': 'You Know, for Search',\n",
      " 'version': {'build_date': '2024-08-05T10:05:34.233336849Z',\n",
      "             'build_flavor': 'default',\n",
      "             'build_hash': '1a77947f34deddb41af25e6f0ddb8e830159c179',\n",
      "             'build_snapshot': False,\n",
      "             'build_type': 'docker',\n",
      "             'lucene_version': '9.11.1',\n",
      "             'minimum_index_compatibility_version': '7.0.0',\n",
      "             'minimum_wire_compatibility_version': '7.17.0',\n",
      "             'number': '8.15.0'}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from elasticsearch import Elasticsearch\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Correct path with double backslashes\n",
    "env_path = os.path.join('e:\\\\', 'Study Space', 'Python Workspace', 'ELastic Search', '.env.local')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Print debugging information\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Environment file path:\", env_path)\n",
    "\n",
    "# Get the LOCALHOST variable\n",
    "LOCALHOST = os.getenv('LOCALHOST')\n",
    "print(\"Raw LOCALHOST value:\", repr(LOCALHOST))\n",
    "\n",
    "# Ensure LOCALHOST is properly processed\n",
    "if not LOCALHOST:\n",
    "    print(\"LOCALHOST not found in environment file.\")\n",
    "    # Fallback to default if not found\n",
    "    LOCALHOST = \"http://localhost:9200/\"\n",
    "else:\n",
    "    print(\"LOCALHOST found in environment file.\")\n",
    "    # Remove quotes if present\n",
    "    LOCALHOST = LOCALHOST.strip('\"')\n",
    "\n",
    "print(\"Processed LOCALHOST value:\", repr(LOCALHOST))\n",
    "\n",
    "# Connect to Elasticsearch\n",
    "try:\n",
    "    es = Elasticsearch([LOCALHOST])\n",
    "    client_info = es.info()\n",
    "    print('Connected to Elasticsearch!')\n",
    "    pprint(client_info.body)\n",
    "except Exception as e:\n",
    "    print(f\"Connection error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.delete(index='test_index', ignore_unavailable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'ZFr20ZMBiPFnoaN1y4Gn',\n",
      " '_index': 'blog_index',\n",
      " '_primary_term': 1,\n",
      " '_seq_no': 0,\n",
      " '_shards': {'failed': 0, 'successful': 1, 'total': 2},\n",
      " '_version': 1,\n",
      " 'result': 'created'}\n"
     ]
    }
   ],
   "source": [
    "blog_post = {\n",
    "  \"title\": \"Elasticsearch Basics\",\n",
    "  \"author\": \"John Doe\",\n",
    "  \"content\": \"Elasticsearch is a search engine based on Lucene.\",\n",
    "  \"published_date\": \"2024-12-16\"\n",
    "}\n",
    "response = es.index(index='blog_index', document=blog_post)\n",
    "pprint(response.body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObjectApiResponse({'_index': 'blog_index', '_id': 'ZFr20ZMBiPFnoaN1y4Gn', '_version': 1, 'result': 'created', '_shards': {'total': 2, 'successful': 1, 'failed': 0}, '_seq_no': 0, '_primary_term': 1})\n"
     ]
    }
   ],
   "source": [
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Elasticsearch Basics', 'author': 'John Doe', 'content': 'Elasticsearch is a search engine based on Lucene.', 'published_date': '2024-12-16'}\n",
      "{'title': 'Elastic Stack', 'author': 'Jane Doe', 'content': 'Elastic Stack is a collection of products that helps you to store, search, analyze, and visualize data.', 'published_date': '2024-12-17'}\n",
      "{'title': 'Kibana', 'author': 'John Doe', 'content': 'Kibana is a data visualization tool that is part of the Elastic Stack.', 'published_date': '2024-12-18'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "json_data = json.load(open('data/blogs.json'))\n",
    "\n",
    "for blog_post in json_data:\n",
    "    print(blog_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: ZloD0pMBiPFnoaN1w4FE is 'created' and is split into 3 shards.\n",
      "Document ID: Z1oD0pMBiPFnoaN1w4GU is 'created' and is split into 3 shards.\n",
      "Document ID: aFoD0pMBiPFnoaN1w4Gg is 'created' and is split into 3 shards.\n"
     ]
    }
   ],
   "source": [
    "def insert_documents(es, index_name = None, data = None):\n",
    "    response = es.index(index=index_name, document=data)\n",
    "    return response\n",
    "\n",
    "es.indices.delete(index='blog_index', ignore_unavailable=True)\n",
    "es.indices.create(index='blog_index', settings={'number_of_shards': 2, 'number_of_replicas': 2})\n",
    "for blog_post in json_data:\n",
    "    response = insert_documents(es, index_name='blog_index', data=blog_post)\n",
    "    print(f\"\"\"Document ID: {response['_id']} is '{\n",
    "        response[\"result\"]}' and is split into {response['_shards']['total']} shards.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **When we have given 2 shards and 2 replicas but it shows 3 shards, WHY?**\n",
    "\n",
    "The reason you are seeing 3 shards in the result even though you set `number_of_shards: 2` is due to Elasticsearch's replication mechanism.\n",
    "### **Understanding Shards and Replicas:**\n",
    "\n",
    "* `Primary Shards`: These are the main shards where the data is stored.\n",
    "* `Replica Shards`: These are copies of the primary shards that provide redundancy and high availability.\n",
    "\n",
    "When you set:\n",
    "\n",
    "```\n",
    "settings={'number_of_shards': 2, 'number_of_replicas': 2}\n",
    "```\n",
    "\n",
    "This means:\n",
    "\n",
    "* number_of_shards = 2 → You will have 2 primary shards.\n",
    "* number_of_replicas = 2 → Each primary shard will have 2 replicas.\n",
    "\n",
    "Thus, the total shards per index are calculated as:\n",
    "    **Total Shards=Primary Shards+(Primary Shards×Replicas)**\n",
    "\n",
    "\n",
    "Substitute the values:\n",
    "    **Total Shards=2+(2×2)=6**\n",
    "\n",
    "### **Why Does It Show 3 Shards per Document?**\n",
    "\n",
    "The message is referring to the shards that were involved in processing the request, not the total shards in the index. When you index a document:\n",
    "\n",
    "1. The document is written to one primary shard.\n",
    "2. Elasticsearch ensures the data is also copied to the replicas to meet the replication requirement.\n",
    "\n",
    "The default behavior involves the following shards:\n",
    "\n",
    "1. Primary Shard for indexing the document.\n",
    "2. Replica Shards for redundancy.\n",
    "\n",
    "Thus, 3 shards (1 primary + 2 replicas) are involved for each document write operation.\n",
    "### **Final Note:**\n",
    "\n",
    "The total shards for the index are 6, but the result shows 3 shards for each document because:\n",
    "\n",
    "* The document was written to 1 primary shard.\n",
    "* The data was replicated to 2 replica shards.\n",
    "\n",
    "If you want fewer shards in total, you can reduce the number_of_replicas. For example:\n",
    "\n",
    "`settings={'number_of_shards': 2, 'number_of_replicas': 1}`\n",
    "\n",
    "This will result in:\n",
    "    **Total Shards=2+(2×1)=4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author': {'fields': {'keyword': {'ignore_above': 256, 'type': 'keyword'}},\n",
      "            'type': 'text'},\n",
      " 'content': {'fields': {'keyword': {'ignore_above': 256, 'type': 'keyword'}},\n",
      "             'type': 'text'},\n",
      " 'published_date': {'type': 'date'},\n",
      " 'title': {'fields': {'keyword': {'ignore_above': 256, 'type': 'keyword'}},\n",
      "           'type': 'text'}}\n"
     ]
    }
   ],
   "source": [
    "index_mapping = es.indices.get_mapping(index='blog_index')\n",
    "pprint(index_mapping[\"blog_index\"][\"mappings\"][\"properties\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eleastic_search_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
